{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Swiggy ChatBot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raBSD1DNVjEV"
      },
      "source": [
        "Import nltk and numpy libraries for the text preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "889ySXORVzBr"
      },
      "source": [
        "import nltk\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsLngqGV8K-"
      },
      "source": [
        "Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApHk4pKmYOC8",
        "outputId": "c9dbeb93-fb85-4e94-faed-ea6926d31ce3"
      },
      "source": [
        "nltk.download('punkt') # downloading model to tokenize message\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxYVU-EDYclF"
      },
      "source": [
        "StopWords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2j70WIXa7Xl",
        "outputId": "03cb6be1-0d38-4ad5-ad7d-0ae7ce28a4fa"
      },
      "source": [
        "nltk.download('stopwords') # downloading stopwords\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jirBKYjwbF1N"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8hC5geScQBF",
        "outputId": "10c3985a-8f8f-4397-df98-914695ee72c3"
      },
      "source": [
        "nltk.download('wordnet') # downloading all lemmas of english language\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMyjdyFZcViG"
      },
      "source": [
        "Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc4p_yVFckEG"
      },
      "source": [
        "def clean_corpus(corpus):\n",
        "    # lowering every word in text\n",
        "    corpus = [ doc.lower() for doc in corpus]\n",
        "    cleaned_corpus = []\n",
        "    stop_words = stopwords.words('english')\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    # iterating over every text\n",
        "    for doc in corpus:\n",
        "        # tokenizing text\n",
        "        tokens = word_tokenize(doc)\n",
        "        cleaned_sentence = []\n",
        "        for token in tokens:\n",
        "            # removing stopwords, and punctuation\n",
        "            if token not in stop_words and token.isalpha():\n",
        "                # applying lemmatization\n",
        "                cleaned_sentence.append(wordnet_lemmatizer.lemmatize(token))\n",
        "                cleaned_corpus.append(' '.join(cleaned_sentence))\n",
        "    return cleaned_corpus"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw7ymAhkcsP2"
      },
      "source": [
        "Intent Classification:\n",
        "Loading the intents from 'intents.json' file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57FJtRV6hYHt"
      },
      "source": [
        "import json\n",
        "with open('/content/intents.json') as file:\n",
        "  intents = json.load(file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3_rGg7UluBK"
      },
      "source": [
        "corpus = []\n",
        "tags = []\n",
        "for intent in intents['intents']:\n",
        "    # taking all patterns in intents to train a neural network\n",
        "    for pattern in intent['patterns']:\n",
        "        corpus.append(pattern)\n",
        "        tags.append(intent['tag'])\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKVUK1tLsRDE"
      },
      "source": [
        "Reshaping the vectors for our neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgDClHGhsS9R"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(np.array(tags).reshape(-1,1))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNiQFsZzser4"
      },
      "source": [
        "We will be developing our neural network for intent classification using the sequential class from tensorflow API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relsv-sdsfsG",
        "outputId": "672ddf46-76d4-43e7-d5aa-da30e5d8d6db"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "model = Sequential([\n",
        "\n",
        "Dense(128, input_shape=(X.shape[1],), activation='relu'), Dropout(0.2), Dense(64, activation='relu'), Dropout(0.2), Dense(y.shape[1], activation='softmax')])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "metrics=['accuracy'])\n",
        "history = model.fit(X.toarray(), y.toarray(), epochs=20, batch_size=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "26/26 [==============================] - 1s 2ms/step - loss: 1.7831 - accuracy: 0.1538 \n",
            "Epoch 2/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.7211 - accuracy: 0.3077\n",
            "Epoch 3/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.6525 - accuracy: 0.3462\n",
            "Epoch 4/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.5892 - accuracy: 0.5385\n",
            "Epoch 5/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.4948 - accuracy: 0.5769\n",
            "Epoch 6/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.3544 - accuracy: 0.8077\n",
            "Epoch 7/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 1.2681 - accuracy: 0.7692\n",
            "Epoch 8/20\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 1.1761 - accuracy: 0.8077\n",
            "Epoch 9/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.9970 - accuracy: 0.9231\n",
            "Epoch 10/20\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.8531 - accuracy: 0.9231\n",
            "Epoch 11/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.7251 - accuracy: 0.9615\n",
            "Epoch 12/20\n",
            "26/26 [==============================] - 0s 1ms/step - loss: 0.6077 - accuracy: 0.9615\n",
            "Epoch 13/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.9615\n",
            "Epoch 15/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.9615\n",
            "Epoch 17/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9615\n",
            "Epoch 20/20\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.1080 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXFB8gI5sjmY"
      },
      "source": [
        "Defining the function to predict intent tag of a particular message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND7gklqJspdi",
        "outputId": "678e33e9-e3e3-424a-d627-7671618221a7"
      },
      "source": [
        "# if prediction for every tag is low, then we want to classify that message as noanswer\n",
        "INTENT_NOT_FOUND_THRESHOLD = 0.40\n",
        "def predict_intent_tag(message):\n",
        "    message = clean_corpus([message])\n",
        "    X_test = vectorizer.transform(message)\n",
        "    y = model.predict(X_test.toarray())\n",
        "    # if probability of all intent is low, classify it as noanswer\n",
        "    if y.max() < INTENT_NOT_FOUND_THRESHOLD:\n",
        "        return 'noanswer'\n",
        "    prediction = np.zeros_like(y[0])\n",
        "    prediction[y.argmax()] = 1\n",
        "    tag = encoder.inverse_transform([prediction])[0][0]\n",
        "    return tag\n",
        "print(predict_intent_tag('How you could help me?'))\n",
        "print(predict_intent_tag('swiggy chat bot'))\n",
        "print(predict_intent_tag('Where\\'s my order'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "noanswer\n",
            "noanswer\n",
            "noanswer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRFhUdBrssyh"
      },
      "source": [
        "Define function to fetch the tag of the intent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSqc9xt2s5vn"
      },
      "source": [
        "import random\n",
        "import time\n",
        "def get_intent(tag):\n",
        "    # to return complete intent from intent tag\n",
        "    for intent in intents['intents']:\n",
        "        if intent['tag'] == tag:\n",
        "            return intent"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cm5voxds9JR"
      },
      "source": [
        "Till now we have fetched the intent of the message received from the user. Now let's define a function to perform a certain action on the basis of the intent classified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XXYR9zDtAHn"
      },
      "source": [
        "def perform_action(action_code, intent):\n",
        "    # funition to perform an action which is required by intent\n",
        "    if action_code == 'CHECK_ORDER_STATUS':\n",
        "        print('\\n Checking database \\n')\n",
        "        time.sleep(2)\n",
        "        order_status = ['in kitchen', 'with delivery executive']\n",
        "        delivery_time = []\n",
        "        return {'intent-tag':intent['next-intent-tag'][0], 'order_status': random.choice(order_status), 'delivery_time': random.randint(10, 30)}\n",
        "    \n",
        "    elif action_code == 'ORDER_CANCEL_CONFIRMATION':\n",
        "        ch = input('BOT: Do you want to continue (Y/n) ?')\n",
        "        if ch == 'y' or ch == 'Y':\n",
        "            choice = 0\n",
        "        else:\n",
        "            choice = 1\n",
        "        return {'intent-tag':intent['next-intent-tag'][choice]}\n",
        "    elif action_code == 'ADD_DELIVERY_INSTRUCTIONS':\n",
        "        instructions = input('Your Instructions: ')\n",
        "        return {'intent-tag':intent['next-intent-tag'][0]}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ay8PvQtEwB"
      },
      "source": [
        "Using all the functions which we have defined to classify intent and perform action accordingly are integrated together to develop our final chatbot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V1d-V1StTq3",
        "outputId": "57d4966e-5155-4d6f-fea0-4307d3385937"
      },
      "source": [
        "while True:\n",
        "    # get message from user\n",
        "    message = input('You: ')\n",
        "    # predict intent tag using trained neural network\n",
        "    tag = predict_intent_tag(message)\n",
        "    # get complete intent from intent tag\n",
        "    intent = get_intent(tag)\n",
        "    # generate random response from intent\n",
        "    response = random.choice(intent['responses'])\n",
        "    print('Bot: ', response)\n",
        "    # check if there's a need to perform some action\n",
        "    if 'action' in intent.keys():\n",
        "        action_code = intent['action']\n",
        "        # perform action\n",
        "        data = perform_action(action_code, intent)\n",
        "        # get follow up intent after performing action\n",
        "        followup_intent = get_intent(data['intent-tag'])\n",
        "        # generate random response from follow up intent\n",
        "        response = random.choice(followup_intent['responses'])\n",
        "        # print randomly selected response\n",
        "        if len(data.keys()) > 1:\n",
        "            print('Bot: ', response.format(**data))\n",
        "        else:\n",
        "            print('Bot: ', response)\n",
        "    # break loop if intent was goodbye\n",
        "    if tag == 'goodbye':\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "Bot:  Hello!\n"
          ]
        }
      ]
    }
  ]
}